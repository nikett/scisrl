{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile='/Users/nikett/quick/sciencesrl-annotation-afresh/results/sandbox_v0.pkl'\n",
    "outfile=infile.replace('.pkl','.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "verb\tmix[3, 4]\n",
      "who\t\n",
      "what\tLight and water\n",
      "when\t\n",
      "where\t\n",
      "whereFrom\t\n",
      "whereTo\t\n",
      "input\tLight and water\n",
      "output\tsugar\n",
      "enable\tform[5, 6]\n",
      "prevent\t\n",
      "==========================\n",
      "verb\tform[5, 6]\n",
      "who\t\n",
      "what\tsugar\n",
      "when\t\n",
      "where\t\n",
      "whereFrom\t\n",
      "whereTo\t\n",
      "input\t\n",
      "output\t\n",
      "enable\t\n",
      "prevent\t\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "# import json\n",
    "# json_response_file=\"/Users/nikett/quick/sciencesrl-annotation-afresh/results/sample_response.json\"\n",
    "# with open(json_response_file) as json_data:\n",
    "#       parsed_json = json.load(json_data)\n",
    "#       ar_dict={}  \n",
    "#       if \"actionRelations\" in parsed_json:\n",
    "#               for ar in parsed_json[\"actionRelations\"]:\n",
    "#                 ar_dict[str(ar[\"source\"]) +\"\\t\"+ ar[\"relation\"]] = str(ar[\"target\"])\n",
    "#       for verbid,p in enumerate(parsed_json[\"actions\"]):\n",
    "#         print(\"==========================\")\n",
    "#         print(\"verb\"+\"\\t\"+p['verb'][\"lemma\"]+\"\"+str(p['verb'][\"span\"]))\n",
    "#         for role in ['who','what', 'when','where','whereFrom','whereTo','input','output','enable','prevent']:\n",
    "#             ar_role = str(verbid)+\"\\t\"+ role\n",
    "#             if ( ar_role in ar_dict):\n",
    "#                 print(role+\"\\t\"+ parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['lemma']+\"\"+ str(parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['span']))\n",
    "#             elif (role in p['questions'] and p['questions'][role]):\n",
    "#                 print(role+\"\\t\"+p['questions'][role][\"phrase\"])\n",
    "#             else:\n",
    "#                 print(role+\"\\t\"+\"\")\n",
    "#                 #TODO write role values to a TSV file\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "sentid: 0\tverbid: 0\n",
      "verb\ttransport[1, 2]\n",
      "who\tPlants\n",
      "what\twater\n",
      "when\t\n",
      "where\tfrom roots to leaves\n",
      "whereFrom\tfrom roots\n",
      "whereTo\tto leaves\n",
      "input\t\n",
      "output\t\n",
      "enable\t\n",
      "prevent\t\n",
      "{'enable': '',\n",
      " 'input': '',\n",
      " 'output': '',\n",
      " 'prevent': '',\n",
      " 'sentid': 0,\n",
      " 'verb': u'transport[1, 2]',\n",
      " 'verbid': 0,\n",
      " 'what': u'water',\n",
      " 'when': '',\n",
      " 'where': u'from roots to leaves',\n",
      " 'whereFrom': u'from roots',\n",
      " 'whereTo': u'to leaves',\n",
      " 'who': u'Plants'}\n",
      "[{'0\\t0': {'prevent': '', 'what': u'water', 'enable': '', 'verbid': 0, 'output': '', 'who': u'Plants', 'when': '', 'verb': u'transport[1, 2]', 'whereFrom': u'from roots', 'whereTo': u'to leaves', 'sentid': 0, 'input': '', 'where': u'from roots to leaves'}}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "from mturk_utils.mturk import pickle_this, unpickle_this\n",
    "\n",
    "def create_result_scisrl(sent_id, assmt):\n",
    "      parsed_json = json.loads(assmt.answers[0][0].fields[0])\n",
    "      ar_dict={}\n",
    "      if \"actionRelations\" in parsed_json:\n",
    "              for ar in parsed_json[\"actionRelations\"]:\n",
    "                ar_dict[str(ar[\"source\"]) +\"\\t\"+ ar[\"relation\"]] = str(ar[\"target\"])\n",
    "      overall_results = {}              \n",
    "      for verbid,p in enumerate(parsed_json[\"actions\"]):\n",
    "        ar_roles = {}\n",
    "        print(\"==========================\")\n",
    "        print(\"sentid: \"+str(sent_id) +\"\\tverbid: \"+ str(verbid))\n",
    "        ar_roles['sentid']=sent_id\n",
    "        ar_roles['verbid']=verbid\n",
    "        print(\"verb\"+\"\\t\"+p['verb'][\"lemma\"]+\"\"+str(p['verb'][\"span\"]))\n",
    "        ar_roles['verb']=p['verb'][\"lemma\"]+\"\"+str(p['verb'][\"span\"])\n",
    "        for role in ['who','what', 'when','where','whereFrom','whereTo','input','output','enable','prevent']:\n",
    "            ar_role = str(verbid)+\"\\t\"+ role\n",
    "            if ( ar_role in ar_dict):\n",
    "                ar_roles[role]=parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['lemma']+\"\"+ str(parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['span'])\n",
    "                print(role+\"\\t\"+ parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['lemma']+\"\"+ str(parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['span']))\n",
    "            elif (role in p['questions'] and p['questions'][role]):\n",
    "                ar_roles[role]=p['questions'][role][\"phrase\"]\n",
    "                print(role+\"\\t\"+p['questions'][role][\"phrase\"])\n",
    "            else:\n",
    "                ar_roles[role]=\"\"\n",
    "                print(role+\"\\t\"+\"\")\n",
    "        pprint(ar_roles)\n",
    "        overall_results[str(sent_id) +\"\\t\"+ str(verbid)] = ar_roles\n",
    "        return overall_results\n",
    "                \n",
    "turk_data = unpickle_this(infile)\n",
    "assignments = [item for sublist in turk_data.values() for item in sublist]\n",
    "assignment_results = [create_result_scisrl(sent_id, ar) for sent_id,ar in enumerate(assignments)]\n",
    "print(assignment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and basic processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from collections import Counter, defaultdict\n",
    "# import json\n",
    "# from mturk_utils.mturk import pickle_this, unpickle_this\n",
    "\n",
    "# def create_result(assmt):\n",
    "#     result = {}\n",
    "#     raw_result = json.loads(assmt.answers[0][0].fields[0])\n",
    "#     result['image_id'] = raw_result['image_url']\n",
    "#     result['object_words_raw'] = sorted([w.replace('_0_','_').replace('None', 'None_0_0') for w in raw_result['description']], key=lambda x: ''.join(x.split('_')[1:]))\n",
    "#     obj_words = [word.split('_')[0] for word in result['object_words_raw']]\n",
    "#     obj_word_location = [word.split('_')[1:] for word in result['object_words_raw']]\n",
    "#     result['object_words'] = obj_words\n",
    "#     result['object_locs'] = obj_word_location\n",
    "#     result['asgmt_id'] = assmt.AssignmentId\n",
    "#     result['hit_id'] = assmt.HITId\n",
    "#     result['worker_id'] = assmt.WorkerId\n",
    "#     return result\n",
    "\n",
    "# turk_data = unpickle_this(infile)\n",
    "# print turk_data\n",
    "# assignments = [item for sublist in turk_data.values() for item in sublist]\n",
    "# assignment_results = [create_result(ar) for ar in assignments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def overlap(row):\n",
    "    for idx in range(row.shape[0]):\n",
    "        if row[idx] == None:\n",
    "            row[idx] = set([])\n",
    "    try:\n",
    "        return list(row[0].intersection(row[1]).union(row[1].intersection(row[2])).union(row[0].intersection(row[2])))\n",
    "    except TypeError:\n",
    "        return {}\n",
    "\n",
    "def distill_objects(obj_idx_list):\n",
    "    combined_objects = []\n",
    "    object_coords = []\n",
    "    \n",
    "    for word_position in obj_idx_list:\n",
    "        split_components = word_position.split('_')\n",
    "        word, sent_n, word_n = split_components[0], int(split_components[1]), int(split_components[2])\n",
    "        object_coords.append((word, sent_n, word_n))\n",
    "    object_coords = sorted(object_coords, key= lambda x: (x[1], x[2]))\n",
    "    combined_objects.append(object_coords[0])\n",
    "    \n",
    "    for idx in range(1, len(object_coords)):\n",
    "        this_word, this_sent, this_wn = object_coords[idx]\n",
    "        last_word, last_sent, last_wn = combined_objects[-1]\n",
    "        if this_sent == last_sent and last_wn + 1 == this_wn:            \n",
    "            combined_objects[-1] = (' '.join([last_word, this_word]), this_sent, this_wn)\n",
    "        else:\n",
    "            combined_objects.append((this_word, this_sent, this_wn))\n",
    "    return [w[1:] for w in combined_objects], [w[0] for w in combined_objects]\n",
    "\n",
    "# Generates spans from words with word ids in text.\n",
    "# e.g., words_input: [u'change_10', u'stay_23', u'the_24', u'same_25', u'may_9']\n",
    "# returns: [u'may_9 change_10', u'stay_23 the_24 same_25']\n",
    "def spans_from(words_input):\n",
    "    # step1: sort words by position\n",
    "    # u'change_10', u'stay_23', u'may_9' => u'may_9', u'change_10', u'stay_23'\n",
    "    sorted_words = words_input\n",
    "    sorted_words.sort(key=lambda x: int(x.split('_')[1]))\n",
    "    # e.g., u'may_9', u'change_10', u'stay_23' (being sorted now)\n",
    "    spans_from_words = \"\"\n",
    "    for i in range(0,len(sorted_words)):\n",
    "        prev_word_position = i-1 if(i>0) else 0\n",
    "        # extract word index (e.g., 23 from stay_23) to compare adjacent (sorted) words\n",
    "        curr_index = (int(sorted_words[i].split('_')[1]))\n",
    "        prev_index = (int(sorted_words[prev_word_position].split('_')[1]))\n",
    "        # insert comma at span boundaries.\n",
    "        spans_from_words += (\",\" if(curr_index - prev_index >1 and i>0) else \" \") + (sorted_words[i])\n",
    "    return spans_from_words.strip().split(',')\n",
    "\n",
    "st4_df = pd.DataFrame(assignment_results)\n",
    "spans = [spans_from(st4_df['object_words_raw'][i]) for i in range(0, st4_df['object_words_raw'].count())]\n",
    "# insert spans as the last column.\n",
    "st4_df['spans'] = spans\n",
    "st4_df.to_csv(outfile, sep='\\t')\n",
    "print \"Result in \" + outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distill good workers.\n",
    "# Find turkers who had proposed \"None\" for most of the hits where others didn't.\n",
    "# st4_df[st4_df['worker_id']=='A3EAOSOUQZFVFS']\n",
    "st4_df[st4_df['worker_id']=='A3EAOSOUQZFVFS']\n",
    "# sorted(st4_df[st4_df['object_words'].apply(lambda x: x[0]) == 'None']['worker_id'])\n",
    "\n",
    "df = st4_df[['image_id', 'spans']]\n",
    "df.to_csv('/tmp/t.tsv', sep='\\t')\n",
    "# my_dict={\n",
    "# 'dummy':{'w':0} # don't know how else to initialize.\n",
    "# }\n",
    "# # my_dict.update({'item4': {'a':2, 'b':3}, 'item5':  {'a':20, 'b':30}})\n",
    "# for s in df[df['image_id']=='7']['spans']:\n",
    "#     my_dict['7'][s]= 1\n",
    "# my_dict\n",
    "\n",
    "\n",
    "# st4_df[['image_id', 'spans']].groupby('image_id').agg(lambda x: ';;'.join(set(x)))\n",
    "# TODO: per image id, aggregate all spans that have support of two or more.\n",
    "# TODO: check against manual gold.\n",
    "#.groupby('image_id').agg(lambda x: ';;'.join(set(x)))\n",
    "\n",
    "# Code in scala and manual filtering is here: /Users/nikett/Documents/work/code/kb-organization/extraction/aristoKB/src/main/scala/eventsAndProcesses/to_be_fixed/predicateid/PredicateIDTurked.scala\n",
    "\n",
    "\n",
    "\n",
    "sentence_annotations = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for i, id in enumerate(sendid):\n",
    "    sentence_annnotations[id][annotation] += 1\n",
    "\n",
    "# filtering step\n",
    "# using dict comprehension here doesn't look good and gets a little complicated\n",
    "\n",
    "for key, value in sentence_annotations.keys():\n",
    "     sentence_annotation[key] = dict((key, value) for key, value in sent_annotation[key].items() if value > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

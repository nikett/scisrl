{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#infile='/Users/nikett/quick/sciencesrl-annotation-afresh/results/sandbox_v2_sample.pkl'\n",
    "# infile='/Users/nikett/quick/sciencesrl-annotation-afresh/results/sandbox_v2_sample.pkl'\n",
    "#infile ='/Users/nikett/quick/sciencesrl-annotation-afresh/results/batch_v2_8k_partial_v2.pkl'\n",
    "# infile='/Users/nikett/quick/sciencesrl-annotation-afresh/results/batch_v5_3.pkl'\n",
    "infile='/Users/nikett/quick/sciencesrl-annotation-afresh/results/batch_v5_3__round3.pkl'\n",
    "outfile=infile.replace('.pkl','.tsv')\n",
    "outfile_highconf=infile.replace('.pkl','.highconf.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4680/4680 [00:01<00:00, 3985.40it/s]\n",
      "100%|██████████| 4680/4680 [00:00<00:00, 20679.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is in  /Users/nikett/quick/sciencesrl-annotation-afresh/results/batch_v5_3__round3.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "from mturk_utils.mturk import pickle_this, unpickle_this\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sanitize_span(span):\n",
    "    return '-'.join([str.strip(str(i)) for i in span.replace('[','').replace(']','').split(',')])\n",
    "\n",
    "def create_result_scisrl(assmt):\n",
    "      parsed_json = json.loads(assmt.answers[0][0].fields[0])\n",
    "      span_sep=\"#\"\n",
    "      ar_dict={}\n",
    "          \n",
    "      try:\n",
    "        dummy_result = parsed_json[\"senttokens\"]\n",
    "      except:\n",
    "        print \"Exception in the parsed json response for sentence with response:\"\n",
    "        pprint(parsed_json)\n",
    "        return {}\n",
    "    \n",
    "      sent_id = parsed_json[\"sentid\"]\n",
    "      tokens = ' '.join([str(t) for t in parsed_json[\"senttokens\"]]) if \"senttokens\" in parsed_json else \"\" \n",
    "      if \"actionRelations\" in parsed_json:\n",
    "              for ar in parsed_json[\"actionRelations\"]:\n",
    "                ar_dict[str(ar[\"source\"]) +\"\\t\"+ ar[\"relation\"]] = str(ar[\"target\"])\n",
    "      overall_results = {}\n",
    "      for verbid,p in enumerate(parsed_json[\"actions\"]):\n",
    "#       print(\"==========================\")\n",
    "#       print(\"sentid: \"+str(sent_id) +\"\\tverbid: \"+ str(verbid))\n",
    "        ar_roles = {}\n",
    "        ar_roles['sentid']=sent_id\n",
    "        ar_roles['senttokens']=tokens\n",
    "        ar_roles['verbid']=verbid\n",
    "        ar_roles['assignment_id']=assmt.AssignmentId\n",
    "        ar_roles['hit_id']=assmt.HITId\n",
    "        ar_roles['worker_id']=assmt.WorkerId\n",
    "#       print(\"verb\"+\"\\t\"+p['verb'][\"lemma\"]+span_sep+sanitize_span(str(p['verb'][\"span\"])))\n",
    "        ar_roles['verb']=str(p['verb'][\"lemma\"])+span_sep+sanitize_span(str(p['verb'][\"span\"]))\n",
    "        for role in ['who','what', 'when','where','whereFrom','whereTo','input','output','enable','prevent']:\n",
    "            ar_role = str(verbid)+\"\\t\"+ role\n",
    "            if (ar_role in ar_dict):\n",
    "                # event-event\n",
    "                ar_roles[role]=parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['lemma']+span_sep+ sanitize_span(str(parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['span']))\n",
    "#               print(role+\"\\t\"+ parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['lemma']+span_sep+ sanitize_span(str(parsed_json[\"actions\"][int(ar_dict[ar_role])]['verb']['span'])))\n",
    "            elif (role in p['questions'] and p['questions'][role]):\n",
    "                # event-args\n",
    "                stri = str(p['questions'][role][\"phrase\"])\n",
    "                if len(stri) > 1:  # answer must have a minimum length of 2.\n",
    "                    ar_roles[role]=stri+span_sep+sanitize_span(str(p['questions'][role][\"span\"]))\n",
    "                else: \n",
    "                    ar_roles[role]=\"\"    \n",
    "#               print(role+\"\\t\"+p['questions'][role][\"phrase\"])+span_sep+sanitize_span(str(p['questions'][role][\"span\"]))\n",
    "            else:\n",
    "                # empty answers\n",
    "                ar_roles[role]=\"\"\n",
    "#               print(role+\"\\t\"+\"\")\n",
    "        overall_results[str(sent_id) +\"\\t\"+ str(verbid)] = ar_roles\n",
    "      return overall_results\n",
    "                \n",
    "turk_data = unpickle_this(infile)\n",
    "assignments = [item for sublist in turk_data.values() for item in sublist]\n",
    "# FIXME sent_id cannot be recovered from the json response-- needs to be updated with sentid?\n",
    "# FIXME need to have a refresh button per verb.\n",
    "# FIXME common output for all the results.\n",
    "assignment_results = [create_result_scisrl(ar) for ar in tqdm(assignments)]\n",
    "role_list=['assignment_id','hit_id','worker_id','sentid','senttokens','verbid','verb', 'who',\n",
    "           'what', 'when','where','whereFrom','whereTo','input','output','enable','prevent']\n",
    "out_handle = open(outfile, 'w')\n",
    "out_handle.write('\\t'.join(role_list) +\"\\n\")\n",
    "for one_result in tqdm(assignment_results):\n",
    "    for k,v in one_result.items():\n",
    "        results = [str(v[role]) for role in role_list]\n",
    "        out_handle.write('\\t'.join(results) +\"\\n\")\n",
    "# print(assignment_results)\n",
    "out_handle.close()\n",
    "print \"Output is in \", outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4116/4116 [02:19<00:00, 26.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is in  /Users/nikett/quick/sciencesrl-annotation-afresh/results/batch_v5_3__round3.highconf.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def elem_from_countmap(e):\n",
    "    # FIXME not sure if we can invoke r1's key (without count) without serializing it.\n",
    "    # the bulb#8-10 spaces 1\n",
    "    return str(e).split('  ')[0] if e else \"\"\n",
    "\n",
    "def longer_overlapping_span(c1,c2):\n",
    "    # 'the battery#14-16', 'from the battery#13-16'\n",
    "    # 'the base of the bulb#5-10', 'the base#5-7'\n",
    "    # 'the base#5-7', 'the base of the bulb#5-10'\n",
    "    min1, max1 = c1.split('#')[-1].split('-')\n",
    "    min2, max2 = c2.split('#')[-1].split('-')\n",
    "    if int(min1)<=int(min2) and int(max1)>=int(max2):\n",
    "        return c1\n",
    "    elif int(min2)<=int(min1) and int(max2)>=int(max1):\n",
    "        return c2\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def longest_overlapping_span(confusions):\n",
    "    # Get the first found subsuming/larger span.\n",
    "    for c1 in confusions:\n",
    "        for c2 in confusions:\n",
    "            if c1!=c2:\n",
    "                result = longer_overlapping_span(c1,c2)\n",
    "                if result:\n",
    "                    return result\n",
    "    return \"\"\n",
    "\n",
    "def high_agreement_during_tie(elems):\n",
    "    #    case 2.1:  the bulb#8-10, , bulb#9-10  [30QQTY5GMKZ9ES6DB76RSS8NBCCU7A]\n",
    "    #    case 2.2:  the light bulb#7-10, light bulb#8-10 , bulb#9-10   [30QQTY5GMKZ9ES6DB76RSS8NBCCU7A]\n",
    "    if not elems or len(elems)==1 or '#' not in elems[0]: \n",
    "        return \"\"\n",
    "    confusing = [elem_from_countmap(elem) for elem in elems]\n",
    "    return longest_overlapping_span(confusing)\n",
    "\n",
    "def high_agreement(a_group, role_list, min_agreement):\n",
    "    # Input: us cool#12-14, keeps us cool#11-14, us cool#12-14\n",
    "    # Output: us cool#12-14\n",
    "    # Step 1: choose clear winner.\n",
    "    # Step 2: otherwise, increase span and choose winner.\n",
    "    #    case 2.1:  the bulb#8-10, , bulb#9-10  [30QQTY5GMKZ9ES6DB76RSS8NBCCU7A]\n",
    "    #    case 2.2:  the light bulb#7-10, light bulb#8-10 , bulb#9-10   [30QQTY5GMKZ9ES6DB76RSS8NBCCU7A]\n",
    "    result = []\n",
    "    for attr in role_list:\n",
    "            v = a_group[attr].value_counts()  # phrase frequency/ agreement within turkers for a slot value.\n",
    "            result_temp_list = [elem_from_countmap(r1) \n",
    "                           for r1 in str(v[v>=min_agreement]).split('\\n') \n",
    "                           if ':' not in r1 and len(r1)>1]\n",
    "            if result_temp_list:\n",
    "                result.append(''.join(result_temp_list))\n",
    "            elif len(v[v>=min_agreement])==0:\n",
    "                v_elems = [v_elem for v_elem in str(v[v>=1]).split('\\n')                            \n",
    "                                                   if ':' not in v_elem and len(v_elem)>1]\n",
    "                result.append(high_agreement_during_tie(v_elems))\n",
    "            else:\n",
    "                result.append(\"\")\n",
    "    return '\\t'.join(result)\n",
    "\n",
    "# see https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas\n",
    "def highconf_data_all(min_agreement=2):\n",
    "    df = pd.read_csv(outfile, sep='\\t')\n",
    "    grouped = df.groupby(['hit_id', 'verbid'])\n",
    "    role_list=['assignment_id','hit_id','worker_id','sentid','senttokens','verbid','verb', 'who',\n",
    "               'what', 'when','where','whereFrom','whereTo','input','output','enable','prevent']\n",
    "    out_handle = open(outfile_highconf, 'w')\n",
    "    out_handle.write('\\t'.join(role_list) +\"\\n\")\n",
    "    for name,a_group in tqdm(grouped):\n",
    "        try:\n",
    "#         if(name[0]=='30QQTY5GMKZ9ES6DB76RSS8NBCCU7A'):\n",
    "            out_handle.write(high_agreement(a_group, role_list, min_agreement) +\"\\n\")\n",
    "        except:\n",
    "            print \"Exception in assignment: \", name\n",
    "    print \"Output is in \", outfile_highconf\n",
    "highconf_data_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and basic processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from collections import Counter, defaultdict\n",
    "# import json\n",
    "# from mturk_utils.mturk import pickle_this, unpickle_this\n",
    "\n",
    "# def create_result(assmt):\n",
    "#     result = {}\n",
    "#     raw_result = json.loads(assmt.answers[0][0].fields[0])\n",
    "#     result['image_id'] = raw_result['image_url']\n",
    "#     result['object_words_raw'] = sorted([w.replace('_0_','_').replace('None', 'None_0_0') for w in raw_result['description']], key=lambda x: ''.join(x.split('_')[1:]))\n",
    "#     obj_words = [word.split('_')[0] for word in result['object_words_raw']]\n",
    "#     obj_word_location = [word.split('_')[1:] for word in result['object_words_raw']]\n",
    "#     result['object_words'] = obj_words\n",
    "#     result['object_locs'] = obj_word_location\n",
    "#     result['asgmt_id'] = assmt.AssignmentId\n",
    "#     result['hit_id'] = assmt.HITId\n",
    "#     result['worker_id'] = assmt.WorkerId\n",
    "#     return result\n",
    "\n",
    "# turk_data = unpickle_this(infile)\n",
    "# print turk_data\n",
    "# assignments = [item for sublist in turk_data.values() for item in sublist]\n",
    "# assignment_results = [create_result(ar) for ar in assignments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def overlap(row):\n",
    "    for idx in range(row.shape[0]):\n",
    "        if row[idx] == None:\n",
    "            row[idx] = set([])\n",
    "    try:\n",
    "        return list(row[0].intersection(row[1]).union(row[1].intersection(row[2])).union(row[0].intersection(row[2])))\n",
    "    except TypeError:\n",
    "        return {}\n",
    "\n",
    "def distill_objects(obj_idx_list):\n",
    "    combined_objects = []\n",
    "    object_coords = []\n",
    "    \n",
    "    for word_position in obj_idx_list:\n",
    "        split_components = word_position.split('_')\n",
    "        word, sent_n, word_n = split_components[0], int(split_components[1]), int(split_components[2])\n",
    "        object_coords.append((word, sent_n, word_n))\n",
    "    object_coords = sorted(object_coords, key= lambda x: (x[1], x[2]))\n",
    "    combined_objects.append(object_coords[0])\n",
    "    \n",
    "    for idx in range(1, len(object_coords)):\n",
    "        this_word, this_sent, this_wn = object_coords[idx]\n",
    "        last_word, last_sent, last_wn = combined_objects[-1]\n",
    "        if this_sent == last_sent and last_wn + 1 == this_wn:            \n",
    "            combined_objects[-1] = (' '.join([last_word, this_word]), this_sent, this_wn)\n",
    "        else:\n",
    "            combined_objects.append((this_word, this_sent, this_wn))\n",
    "    return [w[1:] for w in combined_objects], [w[0] for w in combined_objects]\n",
    "\n",
    "# Generates spans from words with word ids in text.\n",
    "# e.g., words_input: [u'change_10', u'stay_23', u'the_24', u'same_25', u'may_9']\n",
    "# returns: [u'may_9 change_10', u'stay_23 the_24 same_25']\n",
    "def spans_from(words_input):\n",
    "    # step1: sort words by position\n",
    "    # u'change_10', u'stay_23', u'may_9' => u'may_9', u'change_10', u'stay_23'\n",
    "    sorted_words = words_input\n",
    "    sorted_words.sort(key=lambda x: int(x.split('_')[1]))\n",
    "    # e.g., u'may_9', u'change_10', u'stay_23' (being sorted now)\n",
    "    spans_from_words = \"\"\n",
    "    for i in range(0,len(sorted_words)):\n",
    "        prev_word_position = i-1 if(i>0) else 0\n",
    "        # extract word index (e.g., 23 from stay_23) to compare adjacent (sorted) words\n",
    "        curr_index = (int(sorted_words[i].split('_')[1]))\n",
    "        prev_index = (int(sorted_words[prev_word_position].split('_')[1]))\n",
    "        # insert comma at span boundaries.\n",
    "        spans_from_words += (\",\" if(curr_index - prev_index >1 and i>0) else \" \") + (sorted_words[i])\n",
    "    return spans_from_words.strip().split(',')\n",
    "\n",
    "st4_df = pd.DataFrame(assignment_results)\n",
    "spans = [spans_from(st4_df['object_words_raw'][i]) for i in range(0, st4_df['object_words_raw'].count())]\n",
    "# insert spans as the last column.\n",
    "st4_df['spans'] = spans\n",
    "st4_df.to_csv(outfile, sep='\\t')\n",
    "print \"Result in \" + outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distill good workers.\n",
    "# Find turkers who had proposed \"None\" for most of the hits where others didn't.\n",
    "# st4_df[st4_df['worker_id']=='A3EAOSOUQZFVFS']\n",
    "st4_df[st4_df['worker_id']=='A3EAOSOUQZFVFS']\n",
    "# sorted(st4_df[st4_df['object_words'].apply(lambda x: x[0]) == 'None']['worker_id'])\n",
    "\n",
    "df = st4_df[['image_id', 'spans']]\n",
    "df.to_csv('/tmp/t.tsv', sep='\\t')\n",
    "# my_dict={\n",
    "# 'dummy':{'w':0} # don't know how else to initialize.\n",
    "# }\n",
    "# # my_dict.update({'item4': {'a':2, 'b':3}, 'item5':  {'a':20, 'b':30}})\n",
    "# for s in df[df['image_id']=='7']['spans']:\n",
    "#     my_dict['7'][s]= 1\n",
    "# my_dict\n",
    "\n",
    "\n",
    "# st4_df[['image_id', 'spans']].groupby('image_id').agg(lambda x: ';;'.join(set(x)))\n",
    "# TODO: per image id, aggregate all spans that have support of two or more.\n",
    "# TODO: check against manual gold.\n",
    "#.groupby('image_id').agg(lambda x: ';;'.join(set(x)))\n",
    "\n",
    "# Code in scala and manual filtering is here: /Users/nikett/Documents/work/code/kb-organization/extraction/aristoKB/src/main/scala/eventsAndProcesses/to_be_fixed/predicateid/PredicateIDTurked.scala\n",
    "\n",
    "\n",
    "\n",
    "sentence_annotations = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for i, id in enumerate(sendid):\n",
    "    sentence_annnotations[id][annotation] += 1\n",
    "\n",
    "# filtering step\n",
    "# using dict comprehension here doesn't look good and gets a little complicated\n",
    "\n",
    "for key, value in sentence_annotations.keys():\n",
    "     sentence_annotation[key] = dict((key, value) for key, value in sent_annotation[key].items() if value > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
